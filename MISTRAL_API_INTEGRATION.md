# Int√©gration de l'API Mistral

## Configuration

La cl√© API Mistral a √©t√© ajout√©e au fichier `.env`:

```
VITE_MISTRAL_API_KEY=9rLgitb0iaYKdmdRzrkQhuAOBLldeJrj
```

## Architecture

### Routeur LLM (`src/services/llmRouter.ts`)

Le syst√®me utilise un routeur intelligent (`llmRouter`) qui:
- S√©lectionne automatiquement Mistral comme fournisseur par d√©faut
- Utilise le mod√®le `mistral-large-latest`
- Endpoint: `https://api.mistral.ai/v1/chat/completions`

### Configuration Mistral

```typescript
const MISTRAL_CONFIG = {
  endpoint: 'https://api.mistral.ai/v1/chat/completions',
  key: import.meta.env.VITE_MISTRAL_API_KEY,
  model: 'mistral-large-latest'
};
```

## Composants utilisant Mistral

### 1. PromptGeneratorSupabase
- G√©n√©ration de prompts bas√©e sur des crit√®res
- Utilise `llmRouter.selectLLM()` et `llmRouter.callLLM()`

### 2. PromptImprovementSupabase
- Am√©lioration de prompts via MetaPromptOptimizer
- Optimisation r√©flexive via HierarchicalReflectiveOptimizer
- Les deux utilisent le routeur LLM

### 3. MetaPromptOptimizer
- Optimisation meta-cognitive des prompts
- Appelle `llmRouter.selectLLM()` et `llmRouter.callLLM()`

### 4. HierarchicalReflectiveOptimizer
- Optimisation it√©rative par analyse des √©checs
- Appelle `llmRouter.selectLLM()` et `llmRouter.callLLM()`

## Fonctionnalit√©s

### S√©lection Automatique
```typescript
async selectLLM(isAuthenticated: boolean, userHasCredits: boolean): Promise<LLMConfig> {
  // Utilise Mistral pour tous les utilisateurs
  return {
    provider: 'mistral',
    model: MISTRAL_CONFIG.model,
    apiKey: MISTRAL_CONFIG.key,
    endpoint: MISTRAL_CONFIG.endpoint,
    useEdgeFunction: false
  };
}
```

### Appel API Mistral
```typescript
async callMistral(request: LLMRequest): Promise<LLMResponse> {
  // Validation de la cl√©
  if (!MISTRAL_CONFIG.key) {
    throw new Error('Cl√© API Mistral manquante');
  }

  // Appel avec timeout de 45 secondes
  const response = await fetch(MISTRAL_CONFIG.endpoint, {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${MISTRAL_CONFIG.key}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      model: MISTRAL_CONFIG.model,
      messages: request.messages,
      temperature: request.temperature || 0.7,
      max_tokens: request.maxTokens || 8000,
      stream: false
    })
  });

  return parsedResponse;
}
```

## Gestion des Erreurs

### Erreurs g√©r√©es:
1. **Cl√© manquante**: Message d'erreur clair
2. **Timeout**: 45 secondes max, message explicite
3. **Cr√©dits √©puis√©s**: D√©tection du code HTTP 402
4. **Erreurs API**: Parsing et affichage des messages d'erreur

### Exemple de gestion:
```typescript
if (!response.ok) {
  const errorData = await response.json().catch(() => ({}));

  if (response.status === 402) {
    throw new Error('La cl√© API Mistral n\'a plus de cr√©dits disponibles.');
  }

  throw new Error(`Erreur API Mistral: ${response.status} - ${errorData.error?.message}`);
}
```

## Logging et Monitoring

Le syst√®me inclut un logging d√©taill√©:
```typescript
console.log('üîó Appel Mistral API...');
console.log(`‚úÖ Mistral API r√©ponse re√ßue en ${latency}ms`);
console.log('üéØ Configuration LLM s√©lectionn√©e:', llmConfig);
```

## Param√®tres de Requ√™te

### Par d√©faut:
- **Temperature**: 0.7 (√©quilibre cr√©ativit√©/pr√©cision)
- **Max Tokens**: 8000
- **Stream**: false (r√©ponse compl√®te)

### Personnalisables:
```typescript
const response = await llmRouter.callLLM(config, {
  messages: [...],
  temperature: 0.8,  // Personnalisable
  maxTokens: 4000    // Personnalisable
});
```

## Utilisation dans les Composants

### Exemple - G√©n√©ration de prompt:
```typescript
const isAuthenticated = !!user;
const userHasCredits = (credits?.remaining_credits || 0) > 0;

const llmConfig = await llmRouter.selectLLM(isAuthenticated, userHasCredits);

const response = await llmRouter.callLLM(llmConfig, {
  messages: [
    { role: 'system', content: systemPrompt },
    { role: 'user', content: userPrompt }
  ],
  temperature: 0.7,
  maxTokens: 8000
});

const generatedPrompt = response.content;
```

## Format de R√©ponse

```typescript
interface LLMResponse {
  content: string;                    // Contenu g√©n√©r√©
  usage: {
    prompt_tokens: number;            // Tokens du prompt
    completion_tokens: number;        // Tokens de la compl√©tion
    total_tokens: number;             // Total
  };
  model: string;                      // Mod√®le utilis√©
  provider: string;                   // Fournisseur (mistral)
}
```

## Modes de G√©n√©ration

Le syst√®me adapte les prompts selon les cr√©dits disponibles:

### Mode Free (‚â§10 cr√©dits)
- Max 150 tokens
- Structure ultra-concise
- Pas d'exemples

### Mode Basic (11-50 cr√©dits)
- Max 300 tokens
- Structure √©quilibr√©e
- Exemples limit√©s

### Mode Premium (>50 cr√©dits)
- Max 600 tokens
- Structure compl√®te
- M√©thodologie d√©taill√©e

## Avantages de Mistral

1. **Performance**: R√©ponses rapides (<5s g√©n√©ralement)
2. **Qualit√©**: Excellent pour la g√©n√©ration en fran√ßais
3. **Co√ªt**: √âconomique par rapport aux alternatives
4. **Fiabilit√©**: API stable et bien document√©e
5. **Flexibilit√©**: Support de diff√©rents types de t√¢ches

## Fallback et R√©silience

Si Mistral √©choue:
- Gestion d'erreur avec messages clairs
- Toast notifications pour l'utilisateur
- Logging d√©taill√© pour le d√©bogage
- Pas de fallback automatique (pour garantir la coh√©rence)

## S√©curit√©

- Cl√© API stock√©e dans les variables d'environnement
- Pas d'exposition de la cl√© c√¥t√© client (sauf dans les headers de requ√™te)
- Validation de la pr√©sence de la cl√© avant chaque appel
- Gestion des erreurs sans exposer la cl√©

## Notes Importantes

1. **Red√©marrage requis**: Apr√®s modification du `.env`, red√©marrer le serveur de d√©veloppement
2. **Cr√©dits**: Surveiller la consommation via le dashboard Mistral
3. **Rate Limiting**: Respecter les limites de l'API (g√©r√© automatiquement)
4. **Cache**: Pas de cache impl√©ment√©, chaque requ√™te est envoy√©e √† l'API

## Test de l'Int√©gration

Pour tester que Mistral fonctionne:
1. Ouvrir l'application
2. Aller sur la page de g√©n√©ration de prompts
3. Remplir le formulaire
4. Cliquer sur "G√©n√©rer"
5. V√©rifier dans la console: `üéØ Configuration LLM s√©lectionn√©e: { provider: 'mistral' }`
6. V√©rifier le r√©sultat g√©n√©r√©

## D√©pannage

### Erreur "Cl√© API manquante"
- V√©rifier que `VITE_MISTRAL_API_KEY` est dans `.env`
- Red√©marrer le serveur de d√©veloppement

### Erreur 402 "Cr√©dits √©puis√©s"
- V√©rifier le solde sur le dashboard Mistral
- Recharger des cr√©dits si n√©cessaire

### Timeout
- V√©rifier la connexion internet
- V√©rifier que l'API Mistral est accessible

## Prochaines √âtapes

- Monitorer l'utilisation des tokens
- Impl√©menter un cache pour les prompts fr√©quents
- Ajouter des m√©triques de performance
- Optimiser les system prompts pour r√©duire les tokens
